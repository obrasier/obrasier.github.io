```python
import pandas as pd
import numpy as np
import glob
```

This is a follow-up post from my

Okay we've downloaded the files we need from [this dataset](https://www.kaggle.com/cclayford/cricinfo-statsguru-data/data). I just run these bash commands to rename the files for easier shell handling <br> 

find *.csv -exec rename 's/\s/_/g' {} \;

Then to make all the files lowercase:
find *.csv -exec rename 'y/A-Z/a-z/' {} \;

To fix up the headings, we do the same for the first line in all the csv files.

1s/.*/\L&/ < on the first line, make sure all characters are lowercase \L
1s/ /_/g ... on the first line, replace all spaces with underscores, globally

find *.csv -exec sed -i '1s/.*/\L&/;1s/ /_&/' {} \;

Okay that's it. We've done the file processing, now to load them all into jupyter. 

I'm just going to be looking at the Men's test individual data. But you could do the same for the women, ODIs, etc. Just load those into the dataframe instead.


```python
all_files = glob.glob('data/men_test_player_innings_stats_-_*.csv')

# low_memory=False just tries to figure out the data type of large files.
# we don't really need this, because we'll fix these later.
df = pd.concat((pd.read_csv(f, low_memory=False) for f in all_files))

```

There's a few things we should do before we can start processing the data.
- drop the duplicate rows
- change the type of data to the appropriate data type in each column (I *definitely* knew about this before starting the post, yep, *definitely*)
- Sort by date - which will be a handy default


```python
df = df.drop_duplicates()

int_cols = ['innings_runs_scored_num', 'innings_minutes_batted', 
            'innings_batted_flag', 'innings_not_out_flag', 'innings_bowled_flag',
            'innings_balls_faced', 'innings_boundary_fours',
            'innings_boundary_sixes', 'innings_maidens_bowled',
            'innings_runs_conceded', 'innings_wickets_taken',
            '4_wickets', '5_wickets', '10_wickets'
           ]

for col in int_cols:
    # use to_numeric to convery the - to NaN (not-a-number)
    # "Int32" has mixed NaNs and integers
    df[col] = pd.to_numeric(df[col], errors='coerce').astype("Int32")
    

df.innings_economy_rate = pd.to_numeric(df.innings_economy_rate, errors='coerce')
df.innings_date = pd.to_datetime(df.innings_date, infer_datetime_format=True)
df = df.sort_values(by=['innings_date', 'innings_number'])
df.reset_index(drop=True, inplace=True)
all_players = df
```

Okay so last blog we got it working. But it takes about ~5 minutes to run a query. Too fucking slow. We need to stop fucking around and make it run faster. The reason why it was running slow previously because I was doing it in a dumb way. Here was the method before: <br>

- get a separate dataframe of all the players
- for each player of those, filter the original data for each player and then calculate the max consecutive number of matches
- compile the results

The reason this was slow is that each players innings could be anywhere in the dataframe. Instead, we sort the data by player, date, innings number (in that order). Since the data is pre-sorted, it's much faster to mark every batting/bowling stat as matching the criteria. Then split up the dataframe by each player and do it that way. It also returns the index from the original dataset so it's easy to lookup the innings where the data came from. 

With all of those optimisations, queries now take ~15 seconds. Not actually as fast as I was hoping, but much better than going away to boil the kettle, come back and it's still not bloody done. Good-e-fucking-nuf mate.

Here's the faster version below. It also supports the same player matching a criteria multiple times, unlike the crappy old version.



```python
def get_max_consecutive(group):
    # this works for each player
    # first get all matches and compare with a shifted version to how many are the same in a row
    df_bool = group['criteria_match'] != group['criteria_match'].shift()
    # take the cumulative sum of the matches in a row to get the number of each
    df_cumsum = df_bool.cumsum()
    # then groupby the size of each group
    search_groups = group.groupby(df_cumsum)
    
    # the above will give us the size, but we also want the raw data out of each
    # so below we are getting the indexes from the original dataset so we can look them up later
    
    # what we want is the longest group where criteria_match == True
    # make a dictionary with the key the name, but store each length
    lengths = {n: len(g) for n,g in search_groups if g['criteria_match'].all() == True}
    try:
        # m is the maximum number of matches
        m = max(lengths.values())
    except ValueError:
        # if there are no matches, return 0 and not-a-number
        return pd.DataFrame({'maximum': [0], 'num_results':0, 'indexes':[np.nan]})
    # get a list of groups where the match is the players maximum value
    # includes if a player matches multiple times
    max_groups_list = [search_groups.get_group(key) for key,val in lengths.items() if val == m]
    num_items = len(max_groups_list)
    return pd.DataFrame({'maximum': [m]*num_items, 
                         'num_results': [num_items]*num_items, 
                         'indexes': [g['index'].values for g in max_groups_list]})
    
def get_all_consecutive(criteria, data, sort_order=['innings_player', 'innings_date', 'innings_number']):
    data = data.sort_values(by=sort_order)
    data.reset_index(inplace=True)
    # drop all columns where the values were after doesn't exist
    all_columns = list(data.columns.values)
    for c in all_columns:
        if c in criteria:
            data = data[data[c] != np.nan]
    # find all matches and set them to true, set non-matches to False
    data.loc[data.eval(criteria), 'criteria_match'] = True
    data['criteria_match'].fillna(False, inplace=True)
    # group by all the search item, and then actually do the check for each one.
    search_item = all_columns[1]
    grp = data.groupby(search_item)
    results = grp.apply(get_max_consecutive)
    return results
    
    
def get_most_consecutive_individual(criteria, data=all_players.copy(), sort_order=['innings_player', 'innings_date', 'innings_number']):
    results = get_all_consecutive(criteria, data, sort_order)
    # get the maximum result
    max_value = results['maximum'].max()
    # get all matches to the maximum
    max_results = results[results['maximum'] == max_value]
    # get the matches out of the original dataset to get the innings
    result_data = data.reindex(np.concatenate(max_results.indexes.values))
    return max_value, result_data
```

Read the code comments for explanations of what each part is doing. I tried to make it clear. I'll tell you know, that section took me a four or five days on and off. For like 20 lines of code, and most of those I'd written already. What a brainfuck. Okay, let's try this bloody thing out!

The good thing about this system is, we can add as many conditions as we like. So what's the most number of matches where a player has scored over 60 and was also not_out. We can search by using the column heading and a condition:


```python
%%time
criteria = 'innings_runs_scored_num > 60 and innings_not_out_flag == 1'
max_value, results = get_most_consecutive_individual(criteria)
print(max_value)
cols_to_print = ['innings_player', 'innings_runs_scored', 'opposition' ,'ground', 'innings_date', 
                 'innings_minutes_batted', 'innings_number']
print(results[cols_to_print])
```

    3
           innings_player innings_runs_scored    opposition         ground  \
    126481      JH Kallis                 61*   v Australia         Durban   
    128241      JH Kallis                 75*  v Bangladesh    East London   
    128394      JH Kallis                139*  v Bangladesh  Potchefstroom   
    176275    LRPL Taylor                173*    v Zimbabwe       Bulawayo   
    176562    LRPL Taylor                124*    v Zimbabwe       Bulawayo   
    176606    LRPL Taylor                 67*    v Zimbabwe       Bulawayo   
    126787  S Chanderpaul                 67*       v India  Port of Spain   
    126898  S Chanderpaul                101*       v India     Bridgetown   
    126985  S Chanderpaul                136*       v India      St John's   
    134993  S Chanderpaul                101*  v Bangladesh       Kingston   
    135332  S Chanderpaul                128*     v England         Lord's   
    135376  S Chanderpaul                 97*     v England         Lord's   
    149100  S Chanderpaul                107*   v Australia    North Sound   
    149145  S Chanderpaul                 77*   v Australia    North Sound   
    149254  S Chanderpaul                 79*   v Australia     Bridgetown   
    170381  S Chanderpaul                 85*  v Bangladesh      Kingstown   
    170468  S Chanderpaul                 84*  v Bangladesh     Gros Islet   
    170512  S Chanderpaul                101*  v Bangladesh     Gros Islet   
    
           innings_date  innings_minutes_batted innings_number  
    126481   2002-03-15                     159              4  
    128241   2002-10-18                     185              1  
    128394   2002-10-25                     266              2  
    176275   2016-07-28                     365              2  
    176562   2016-08-06                     240              1  
    176606   2016-08-06                     125              3  
    126787   2002-04-19                     260              4  
    126898   2002-05-02                     365              2  
    126985   2002-05-10                     675              2  
    134993   2004-06-04                     271              2  
    135332   2004-07-22                     383              2  
    135376   2004-07-22                     231              4  
    149100   2008-05-30                     352              2  
    149145   2008-05-30                     336              4  
    149254   2008-06-12                     226              2  
    170381   2014-09-05                     302              1  
    170468   2014-09-13                     268              1  
    170512   2014-09-13                     173              3  
    CPU times: user 18.2 s, sys: 19.5 ms, total: 18.2 s
    Wall time: 18.2 s


So the most in test history is 3. But Chanderpaul has done that *four fucking times*. Also, I can't tell you how satisfying it is when my queries run in a few seconds. Lets try another query, what's most number of times in a row someone has batted more than 200 minutes, but not scored a century?


```python
criteria = 'innings_runs_scored_num < 100 and innings_minutes_batted > 200'
max_value, results = get_most_consecutive_individual(criteria)
print(max_value)
print(results[cols_to_print])
```

    search_item is innings_player
    4
           innings_player innings_runs_scored      opposition         ground  \
    73183        DI Gower                  74         v India        Kolkata   
    73338        DI Gower                  64         v India        Chennai   
    73382        DI Gower                  85         v India         Kanpur   
    73535        DI Gower                  89     v Sri Lanka  Colombo (PSS)   
    180059       JA Raval                  80  v South Africa     Wellington   
    180170       JA Raval                  88  v South Africa       Hamilton   
    182351       JA Raval                  42   v West Indies     Wellington   
    182567       JA Raval                  84   v West Indies       Hamilton   
    103824       RG Twose                  36         v India        Cuttack   
    104275       RG Twose                  59      v Pakistan   Christchurch   
    104351       RG Twose                 51*      v Pakistan   Christchurch   
    104682       RG Twose                  42      v Zimbabwe       Hamilton   
    149145  S Chanderpaul                 77*     v Australia    North Sound   
    149254  S Chanderpaul                 79*     v Australia     Bridgetown   
    149300  S Chanderpaul                  50     v Australia     Bridgetown   
    150729  S Chanderpaul                  76   v New Zealand        Dunedin   
    
           innings_date  innings_minutes_batted innings_number  
    73183    1982-01-01                     235              3  
    73338    1982-01-13                     215              2  
    73382    1982-01-30                     257              1  
    73535    1982-02-17                     256              2  
    180059   2017-03-16                     253              3  
    180170   2017-03-25                     396              2  
    182351   2017-12-01                     220              2  
    182567   2017-12-09                     231              1  
    103824   1995-11-08                     204              2  
    104275   1995-12-08                     210              2  
    104351   1995-12-08                     257              4  
    104682   1996-01-13                     227              1  
    149145   2008-05-30                     336              4  
    149254   2008-06-12                     226              2  
    149300   2008-06-12                     201              4  
    150729   2008-12-11                     281              2  


Our mate Chanderpaul again. Never thought of him as a slow-poke like a Gower. But there you go. You may be wondering what the `sort_order` is for. Well, individual/time is the most likely order. So the default is to use 
`['innings_player', 'innings_date', 'innings_number']` as the search order. 

What if we wanted to instead, find out who did something at a particular ground, or against a particular opposition. Let's see the most innings in Kolkata where bowlers have taken at least 3 wickets.


```python
# double quotes around the text (string) is important
criteria = 'innings_wickets_taken >= 3 and ground == "Kolkata"'

# sorting by player, ground, data, number groups the players together
sort_order = ['innings_player', 'ground', 'innings_date', 'innings_number']

# the reason for get_all instead of get_most is sometimes we want to know all the top players that satisfy the condition
# we the get_most function gets the innings from the 'indexes' column
# look at the code inside the get_most function to see how it looks up the indexes from the original dataset
results = get_all_consecutive(criteria, df.copy(), sort_order)
results.sort_values(by=['maximum'], ascending=False, inplace=True)
print(results.head(10))
```

    search_item is innings_player
                        maximum  num_results  \
    innings_player                             
    Mohammed Shami   0        5            1   
    J Srinath        0        4            1   
    A Kumble         0        4            1   
    R Benaud         0        4            1   
    BS Chandrasekhar 0        3            1   
    AME Roberts      0        3            1   
    Ghulam Ahmed     0        3            1   
    Harbhajan Singh  0        3            1   
    WW Hall          0        2            1   
    SA Durani        0        2            1   
    
                                                         indexes  
    innings_player                                                
    Mohammed Shami   0  [167291, 167335, 177211, 177255, 182106]  
    J Srinath        0          [111551, 111594, 114439, 114483]  
    A Kumble         0          [136762, 138136, 138180, 146913]  
    R Benaud         0              [34206, 34250, 38422, 38465]  
    BS Chandrasekhar 0                     [48570, 56407, 56452]  
    AME Roberts      0                     [59873, 59918, 77379]  
    Ghulam Ahmed     0                     [28440, 34195, 34240]  
    Harbhajan Singh  0                  [121652, 121718, 128470]  
    WW Hall          0                            [36567, 36611]  
    SA Durani        0                            [40953, 40997]  


What about in a country? Let's see which bowlers have taken 3 or more wickets in a row in England. To get a whole country, all we really have to go on is the grounds, so we can build up a query to check `or` each of those grounds. Like below:


```python
# we need to get all the english grounds out of our dataset. I might have missed some.
english_grounds = ["Lord's", "Birmingham", "Manchester", "The Oval", "Sheffield", "Nottingham", "Leeds"]

# this is how you can build it up programmatically, start with the beginning of the query
criteria = '''innings_wickets_taken >= 3 and (ground == '''

# this next line is tricky, have to wrap each item in double quotes ""
criteria += ' or ground == '.join(f'"{g}"' for g in english_grounds)

# finish it off by closing the brackets
criteria += ')'
# print so you see that the final criteria looks like
print(criteria)

# sorting by player, ground, data, number groups the players together
sort_order = ['innings_player', 'ground', 'innings_date', 'innings_number']

results = get_all_consecutive(criteria, df.copy(), sort_order)
# print(results.dropna())
results.sort_values(by=['maximum'], ascending=False, inplace=True)

# select 'maximum' so you don't have to see the indexes
print(results.head(10)['maximum'])
```

    innings_wickets_taken >= 3 and (ground == "Lord's" or ground == "Birmingham" or ground == "Manchester" or ground == "The Oval" or ground == "Sheffield" or ground == "Nottingham" or ground == "Leeds")
    search_item is innings_player
    innings_player   
    GD McGrath      0    6
    RR Lindwall     0    6
    T Richardson    0    6
    GP Swann        0    6
    SK Warne        0    6
    D Gough         0    6
    IT Botham       0    5
    AV Bedser       0    5
    SF Barnes       0    5
    CA Walsh        0    5
    Name: maximum, dtype: int64


Ooh ahh, Glenn McGrath. Okay, now we want to apply our faster-and-improved-consecutive-search-function to the team data we scraped from statsguru. But we're going to make a `_team` function, but since the data is the same, we can figure out from the column headings if the query is for individuals or for teams, so we shouldn't need to pass that data.


```python
all_innings_file = 'data/all_test_innings.csv'
all_innings = pd.read_csv(all_innings_file)
all_innings.start_date = pd.to_datetime(all_innings.start_date, infer_datetime_format=True)
all_innings.runs = pd.to_numeric(all_innings.runs, errors='coerce').astype("Int64")

```


```python
def get_most_consecutive_team(criteria, data=all_innings.copy(), sort_order=['team', 'start_date', 'innings']):
    data = all_innings.copy()
    results = get_all_consecutive(criteria, data, sort_order)
    # get the maximum result
    max_value = results['maximum'].max()
    # get all matches to the maximum
    max_results = results[results['maximum'] == max_value]
    # get the matches out of the original dataset to get the innings
    result_data = data.reindex(np.concatenate(max_results.indexes.values))
    return max_value, result_data
```

Then we want to write the same function we had for the individual, but include the team stats. The only different between the two, is the dataset, and the sort order. I might consolidate the two later. Whatever. Okay, again let's find the team that scored the most innings in a row over 300 (when they also got all out).


```python
criteria = 'runs < 150 and all_out_flag == 1'
max_value, results = get_most_consecutive_team(criteria)
print(max_value)
print(results)

```

    search_item is team
    9
              team score  runs  overs  balls_per_over   rpo  lead  innings result  \
    97   Australia    42    42   37.3               4  1.66   -71        2   lost   
    99   Australia    82    82   69.2               4  1.76  -126        4   lost   
    100  Australia   116   116   71.2               4  2.43   116        1    won   
    102  Australia    60    60   29.2               4  3.05   123        3    won   
    104  Australia    80    80   90.3               4  1.32    80        1   lost   
    106  Australia   100   100   69.2               4  2.15  -137        3   lost   
    108  Australia    81    81   52.2               4  2.31   -91        2   lost   
    109  Australia    70    70   31.1               4  3.36   -21        3   lost   
    117  Australia   132   132   86.0               5  1.84   132        1   lost   
    
        opposition      ground start_date  all_out_flag  declared_flag  
    97   v England      Sydney 1888-02-10             1              0  
    99   v England      Sydney 1888-02-10             1              0  
    100  v England      Lord's 1888-07-16             1              0  
    102  v England      Lord's 1888-07-16             1              0  
    104  v England    The Oval 1888-08-13             1              0  
    106  v England    The Oval 1888-08-13             1              0  
    108  v England  Manchester 1888-08-30             1              0  
    109  v England  Manchester 1888-08-30             1              0  
    117  v England      Lord's 1890-07-21             1              0  


Okay so we got what we were after, but we want more! Like a dog who wants a bone, we can never get enough stats. 21 scores under 200 in a row! When the hell were they! The last piece of our puzzle is to get the rows that match, and give us back the original dataframe so we can see where Australia's terrible streak was. Let me tell you now, this is a hard problem.

So it works! We answered the queries, but it's a bit bloody clunky. I'm going to just submit this post now, but I will edit it. Because I'm sure I should have been using [df.query](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) this whole time to make it easier to write generic queries instead of the filtering I've been doing. 


```python

```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-12-d637d207f60e> in <module>
    ----> 1 print(all_teams)
    

    NameError: name 'all_teams' is not defined



```python

```
